{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d503c74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"1\"\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import (\n",
    "    LlavaNextProcessor,\n",
    "    LlavaNextForConditionalGeneration\n",
    ")\n",
    "from IPython.display import display\n",
    "from typing import List, Dict\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5526769d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## configurations\n",
    "model_id : str = \"llava-hf/llava-v1.6-mistral-7b-hf\"\n",
    "seed : int = 42\n",
    "max_new_tokens: int = 500\n",
    "\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eadfb10",
   "metadata": {},
   "source": [
    "## Load the models and processor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2143fcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = LlavaNextProcessor.from_pretrained(model_id)\n",
    "model = LlavaNextForConditionalGeneration.from_pretrained(\n",
    "            model_id,\n",
    "            torch_dtype=torch.float16,  # <--bf16 if supported\n",
    "            low_cpu_mem_usage=True,\n",
    "            device_map=\"auto\",\n",
    "            # quantization_config=quantization_config,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7f60ba",
   "metadata": {},
   "source": [
    "### 1. Test the model inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64d5949",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample = Image.open(\"test_sample.png\")\n",
    "display(test_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed13f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\"},\n",
    "            {\"type\": \"text\", \"text\": \"Describe the image in detail.\"},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "## prepare input\n",
    "prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "inputs = processor(prompt, test_sample, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "\n",
    "## now generate\n",
    "output = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)\n",
    "output = processor.decode(output[0], skip_special_tokens=True)\n",
    "output = output.split(\"[/INST]\")[-1].strip()\n",
    "print(f\"Image description: {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15383dd",
   "metadata": {},
   "source": [
    "# Multimodal Uncertainty Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721ba7a8",
   "metadata": {},
   "source": [
    "## 1. Question-induced Hallucination test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794890f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Question induced hallucination\n",
    "# in the image, the ping pong table is not present. Let's check!\n",
    "\n",
    "question_prompt = \"Is the target picture hanging on the wall in the room with the ping pong table?\"\n",
    "\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\"},\n",
    "            {\"type\": \"text\", \"text\": question_prompt },\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "## prepare input\n",
    "prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "inputs = processor(prompt, test_sample, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "\n",
    "## now generate\n",
    "output = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)\n",
    "output = processor.decode(output[0], skip_special_tokens=True)\n",
    "output = output.split(\"[/INST]\")[-1].strip()\n",
    "\n",
    "# Here I got: \"Yes, the target picture is hanging on the wall in the room with the ping pong table.\"\n",
    "# which, of course, is not correct.\n",
    "print(f\"Question-induced hallucination? The model's answer is :---> {output}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71f871e",
   "metadata": {},
   "source": [
    "## 2. Multimodal Uncertainty Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7024ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_prompt = \"Is the target picture hanging on the wall in the room with the ping pong table? You must answer only with 'Yes','No', or '?=I don't know'.\"\n",
    "\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\"},\n",
    "            {\"type\": \"text\", \"text\": question_prompt },\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "## prepare input\n",
    "prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "inputs = processor(prompt, test_sample, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "\n",
    "## now generate\n",
    "output = model.generate(**inputs, \n",
    "                        max_new_tokens=max_new_tokens, \n",
    "                        do_sample=False,                 \n",
    "                        output_logits=True,  # <-- we want the logits\n",
    "                        return_dict_in_generate=True \n",
    "                        )\n",
    "\n",
    "output_decoded = processor.decode(output[\"sequences\"][0], skip_special_tokens=True)\n",
    "output_string = output_decoded.split(\"[/INST]\")[-1].strip()\n",
    "\n",
    "logits = output[\"logits\"]\n",
    "print(\"Answer of the model: \", output_string) # <--- the model still answers \"Yes\"\n",
    "\n",
    "\n",
    "########## Now let's check the logits\n",
    "for timestep_t in range(len(logits)):\n",
    "    logits_time_t = logits[timestep_t][0]  # -> shape num_vocab\n",
    "    logits_time_t = torch.softmax(logits_time_t, dim=-1)\n",
    "\n",
    "    # get tokens likelihood\n",
    "    top_k = 3\n",
    "    topk_scores, topk_indices = torch.topk(logits_time_t, top_k)\n",
    "    topk_indices = topk_indices.cpu().numpy()\n",
    "\n",
    "    token_likelihood : List[Dict] = []\n",
    "    for i in range(top_k):\n",
    "        decoded: str = processor.decode([topk_indices[i]], skip_special_tokens=True)\n",
    "        likelihood = round(logits_time_t[topk_indices[i]].cpu().item(), 3)\n",
    "        token_likelihood.append((decoded, likelihood))\n",
    "\n",
    "    break\n",
    "\n",
    "print(f\"The top-{top_k} tokens are: {token_likelihood}\")\n",
    "\n",
    "### Compute the normalized entropy estimation\n",
    "tau = 0.75\n",
    "token_likelihood = token_likelihood\n",
    "logits = torch.tensor([logit for _ , logit in token_likelihood])\n",
    "print(f\"Logits: {logits}\")\n",
    "entropy = -torch.sum(logits * torch.log(logits))\n",
    "\n",
    "entropy_max = torch.log(torch.tensor(len(logits)))\n",
    "entropy_normalized = entropy / entropy_max\n",
    "\n",
    "model_certainty = \"Certain\" if entropy_normalized <= tau else \"Uncertain\"\n",
    "print(f\"Model Certainty: Entropy normalized: {entropy_normalized} - Model Certainty: {model_certainty} (tau={tau})\")\n",
    "\n",
    "\n",
    "#### Create an image for clarity\n",
    "fig = go.Figure()\n",
    "tokens = [tokens for tokens , _ in token_likelihood]\n",
    "scores = [scores for _ , scores in token_likelihood]\n",
    "fig.add_trace(go.Bar(x=tokens, y=scores, text=scores, textposition='auto'))\n",
    "\n",
    "img_array = np.array(test_sample)\n",
    "fig.add_layout_image(\n",
    "    dict(\n",
    "        source=test_sample,\n",
    "        xref=\"paper\",\n",
    "        yref=\"paper\",\n",
    "        x=1.05,\n",
    "        y=1,\n",
    "        sizex=0.6,\n",
    "        sizey=1,\n",
    "        yanchor=\"top\",\n",
    "        sizing=\"contain\",\n",
    "        layer=\"above\"\n",
    "    )\n",
    ")\n",
    "\n",
    "wrapped_question = \"<br>\".join(textwrap.wrap(f\"Question: {question_prompt}\", width=100))\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis=dict(domain=[0, 0.7]),\n",
    "      yaxis=dict(domain=[0, 0.85]),\n",
    "    annotations=[\n",
    "        dict(\n",
    "            text=f\"<b>Question:</b> {wrapped_question}\",\n",
    "            xref=\"paper\", yref=\"paper\",\n",
    "            x=0, y=1.15,\n",
    "            showarrow=False,\n",
    "            font=dict(size=14)\n",
    "        ),\n",
    "        dict(\n",
    "            text=f\"<b>Result after Normalized-entropy estimation technique:</b> {model_certainty}\",\n",
    "            xref=\"paper\", yref=\"paper\",\n",
    "            x=0, y=1.0,\n",
    "            showarrow=False,\n",
    "            font=dict(size=14, color=\"gray\"),\n",
    "            align=\"left\"\n",
    "        )\n",
    "    ],\n",
    "    margin=dict(l=50, r=400, t=70, b=50),\n",
    "    width=1000,\n",
    "    height=500,\n",
    ")\n",
    "fig.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vlfm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
